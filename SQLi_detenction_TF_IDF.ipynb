{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL Injection Detenction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from texttable import Texttable\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import hstack\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier, RandomForestClassifier, StackingClassifier\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = pd.read_csv('./Dataset/trainingdata.csv')\n",
    "training.drop_duplicates(inplace=True)\n",
    "\n",
    "testing = pd.read_csv('./Dataset/testingdata.csv')\n",
    "testing.drop_duplicates(inplace=True)\n",
    "\n",
    "data = pd.concat([training, testing], axis=0)\n",
    "data.drop_duplicates(inplace=True)\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "data = data.dropna(subset=['Label', 'Query'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per estrarre le caratteristiche da una query SQL\n",
    "def extract_features(query):\n",
    "    features = {}\n",
    "    features['no_sngle_quts'] = query.count(\"'\")  # Number of single quotes\n",
    "    features['no_dble_quts'] = query.count('\"')  # Number of double quotes\n",
    "    features['no_punctn'] = sum([1 for char in query if char in '!@#$%^&*()-_=+[{]};:\\'\",<.>/?\\\\|`~'])  # Number of punctuations\n",
    "    features['no_sgle_cmnt'] = query.count('--')  # Number of single line comments\n",
    "    features['no_mlt_cmnt'] = query.count('/*') + query.count('*/')  # Number of multi-line comments\n",
    "    features['no_whte_spce'] = query.count(' ')  # Number of white spaces\n",
    "    features['no_nrml_kywrds'] = len(re.findall(r'\\b(select|from|where|insert|delete|update|join|union)\\b', query, re.IGNORECASE))  # Normal keywords\n",
    "    features['no_hmfl_kywrds'] = len(re.findall(r'\\b(exec|shutdown|cmdshell|ascii|hex|char|concat)\\b', query, re.IGNORECASE))  # Harmful keywords\n",
    "    features['no_prctge'] = query.count('%')  # Number of percentage symbols\n",
    "    features['no_log_oprtr'] = len(re.findall(r'\\b(and|or|not)\\b', query, re.IGNORECASE))  # Logical operators\n",
    "    features['no_oprtr'] = sum([1 for char in query if char in '=<>'])  # Number of operators\n",
    "    features['no_null_valus'] = query.lower().count('null')  # Number of null values\n",
    "    features['no_hexdcml_valus'] = len(re.findall(r'0x[0-9a-fA-F]+', query))  # Number of hexadecimal values\n",
    "    features['no_db_info_cmnds'] = len(re.findall(r'\\b(database|information_schema|version)\\b', query, re.IGNORECASE))  # Database information commands\n",
    "    features['no_roles'] = len(re.findall(r'\\b(admin|user|guest)\\b', query, re.IGNORECASE))  # Roles\n",
    "    features['no_ntwr_cmnds'] = len(re.findall(r'\\b(load_file|benchmark|sleep)\\b', query, re.IGNORECASE))  # Network commands\n",
    "    features['no_lanage_cmnds'] = len(re.findall(r'\\b(exec|declare|open|fetch|close|deallocate|prepare|execute)\\b', query, re.IGNORECASE))  # Language commands\n",
    "    features['no_alphabet'] = len(re.findall(r'[a-zA-Z]', query))  # Number of alphabets\n",
    "    features['no_digits'] = len(re.findall(r'\\d', query))  # Number of digits\n",
    "    features['no_spl_chrtr'] = len(re.findall(r'[^a-zA-Z0-9\\s]', query))  # Number of special characters\n",
    "    return features\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    roc_auc = roc_auc_score(y_true, y_pred)\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1, \"conf_matrix\": conf_matrix, \"roc_auc\": roc_auc}\n",
    "\n",
    "# Function to plot confusion matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, title, filename):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel('Predicted Label', fontsize=14)\n",
    "    plt.ylabel('True Label', fontsize=14)\n",
    "    \n",
    "    # Increase font size of the numbers in the confusion matrix\n",
    "    for labels in disp.text_.ravel():\n",
    "        labels.set_fontsize(14)\n",
    "    \n",
    "    plt.savefig(filename, format='png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# Function to plot ROC-AUC Curve\n",
    "def plot_roc_auc_curve(y_true, y_pred_prob, model_name, filename):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC-AUC Curve: {model_name}')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.savefig(filename, format='png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_classification_report(models_stats, filename_prefix):\n",
    "    metrics = ['roc_auc', 'precision', 'recall', 'f1']\n",
    "    models = list(models_stats.keys())\n",
    "    \n",
    "    # Prepare the data\n",
    "    report_data = []\n",
    "    for model, stats in models_stats.items():\n",
    "        report_data.append([stats['roc_auc'], stats['precision'], stats['recall'], stats['f1']])\n",
    "    \n",
    "    # Convert to DataFrame for easier plotting\n",
    "    report_df = pd.DataFrame(report_data, columns=metrics, index=models)\n",
    "\n",
    "    colors = ['#007bff', '#28a745', '#dc3545', '#ffc107']\n",
    "\n",
    "    # Plot all models in one go\n",
    "    ax = report_df.plot(kind='bar', figsize=(14, 8), width=0.7, color=colors, legend=False)  # Adjusted figure size\n",
    "    plt.title(\"Classification Report\", fontsize=16)\n",
    "    plt.xticks(rotation=45, ha='right')  # Rotate labels for better readability\n",
    "    plt.ylabel(\"Score (%)\", fontsize=14)\n",
    "    plt.xticks(fontsize=14)  # Increase font size for x-axis labels\n",
    "    plt.yticks(fontsize=14)  # Increase font size for y-axis labels\n",
    "    \n",
    "    # Set y-axis range for better differentiation\n",
    "    ax.set_ylim(0.8, 1.05)  # Adjust this range as per your needs\n",
    "    \n",
    "    # Annotate the bars with the actual values\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(f'{p.get_height():.4f}', \n",
    "                    (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                    ha='center', va='baseline', \n",
    "                    xytext=(0, -100), \n",
    "                    textcoords='offset points', fontsize=13, rotation=90)\n",
    "    \n",
    "    # Move legend to bottom left corner\n",
    "    plt.legend(loc='lower left')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{filename_prefix}_all_models.png\", format='png', dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estrazione le caratteristiche dalle query\n",
    "data_features = data['Query'].apply(extract_features)\n",
    "data_features_df = pd.DataFrame(data_features.tolist())\n",
    "\n",
    "# Combina le caratteristiche estratte con le etichette\n",
    "df = pd.concat([data_features_df, data['Label']], axis=1)\n",
    "\n",
    "# Separazione delle caratteristiche (X) e le etichette (y)\n",
    "X_numerical = df.drop(columns=['Label'])\n",
    "y = df['Label']\n",
    "\n",
    "# Salva la colonna 'Query' per la successiva creazione delle BoW\n",
    "X_text = data['Query']\n",
    "\n",
    "# Divide i dati in set di addestramento e test mantenendo la colonna 'Query'\n",
    "X_train_numerical, X_test_numerical, X_train_text, X_test_text, y_train, y_test = train_test_split(\n",
    "    X_numerical, X_text, y, test_size=0.4)\n",
    "\n",
    "# Trasforma le caratteristiche testuali in TF-IDF\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=1000)\n",
    "X_train_query = vectorizer.fit_transform(X_train_text)\n",
    "X_test_query = vectorizer.transform(X_test_text)\n",
    "\n",
    "# Combina le caratteristiche numeriche e testuali\n",
    "X_train = hstack((X_train_numerical.values, X_train_query)).tocsr()\n",
    "X_test = hstack((X_test_numerical.values, X_test_query)).tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate of models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm = GradientBoostingClassifier()\n",
    "gbm.fit(X_train, y_train)\n",
    "y_pred_gbm = gbm.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\OneDrive - uniroma1.it\\Desktop\\Uni\\3°Anno uni\\Sicurezza\\SQLi detenction\\venv\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ada = AdaBoostClassifier()\n",
    "ada.fit(X_train, y_train)\n",
    "y_pred_ada = ada.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\OneDrive - uniroma1.it\\Desktop\\Uni\\3°Anno uni\\Sicurezza\\SQLi detenction\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [17:12:02] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "xgb.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 40487, number of negative: 33220\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.220572 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 221934\n",
      "[LightGBM] [Info] Number of data points in the train set: 73707, number of used features: 1020\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.549297 -> initscore=0.197829\n",
      "[LightGBM] [Info] Start training from score 0.197829\n"
     ]
    }
   ],
   "source": [
    "lgbm = LGBMClassifier()\n",
    "lgbm.fit(X_train, y_train)\n",
    "y_pred_lgbm = lgbm.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(max_iter=100000)\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_pred_log_reg = log_reg.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "#kf = KFold(n_splits=5, shuffle=True)\n",
    "#results = cross_validate(log_reg, X_train, y_train, cv=kf, scoring=['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted'])\n",
    "#print(f\"Mean Accuracy: {results['test_accuracy'].mean():.2f}\")\n",
    "#print(f\"Mean Precision: {results['test_precision_weighted'].mean():.2f}\")\n",
    "#print(f\"Mean Recall: {results['test_recall_weighted'].mean():.2f}\")\n",
    "#print(f\"Mean F1 Score: {results['test_f1_weighted'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred_knn = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred_dt = dt.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\OneDrive - uniroma1.it\\Desktop\\Uni\\3°Anno uni\\Sicurezza\\SQLi detenction\\venv\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\hp\\OneDrive - uniroma1.it\\Desktop\\Uni\\3°Anno uni\\Sicurezza\\SQLi detenction\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [17:27:46] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 40487, number of negative: 33220\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.196431 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 221932\n",
      "[LightGBM] [Info] Number of data points in the train set: 73707, number of used features: 1020\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.549297 -> initscore=0.197829\n",
      "[LightGBM] [Info] Start training from score 0.197829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\OneDrive - uniroma1.it\\Desktop\\Uni\\3°Anno uni\\Sicurezza\\SQLi detenction\\venv\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\hp\\OneDrive - uniroma1.it\\Desktop\\Uni\\3°Anno uni\\Sicurezza\\SQLi detenction\\venv\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\hp\\OneDrive - uniroma1.it\\Desktop\\Uni\\3°Anno uni\\Sicurezza\\SQLi detenction\\venv\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\hp\\OneDrive - uniroma1.it\\Desktop\\Uni\\3°Anno uni\\Sicurezza\\SQLi detenction\\venv\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\hp\\OneDrive - uniroma1.it\\Desktop\\Uni\\3°Anno uni\\Sicurezza\\SQLi detenction\\venv\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\hp\\OneDrive - uniroma1.it\\Desktop\\Uni\\3°Anno uni\\Sicurezza\\SQLi detenction\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [17:39:47] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hp\\OneDrive - uniroma1.it\\Desktop\\Uni\\3°Anno uni\\Sicurezza\\SQLi detenction\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [17:39:59] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hp\\OneDrive - uniroma1.it\\Desktop\\Uni\\3°Anno uni\\Sicurezza\\SQLi detenction\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [17:40:10] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hp\\OneDrive - uniroma1.it\\Desktop\\Uni\\3°Anno uni\\Sicurezza\\SQLi detenction\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [17:40:22] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\hp\\OneDrive - uniroma1.it\\Desktop\\Uni\\3°Anno uni\\Sicurezza\\SQLi detenction\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [17:40:34] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 32389, number of negative: 26576\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.153521 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 199298\n",
      "[LightGBM] [Info] Number of data points in the train set: 58965, number of used features: 1020\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.549292 -> initscore=0.197810\n",
      "[LightGBM] [Info] Start training from score 0.197810\n",
      "[LightGBM] [Info] Number of positive: 32389, number of negative: 26576\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.185523 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 201525\n",
      "[LightGBM] [Info] Number of data points in the train set: 58965, number of used features: 1020\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.549292 -> initscore=0.197810\n",
      "[LightGBM] [Info] Start training from score 0.197810\n",
      "[LightGBM] [Info] Number of positive: 32390, number of negative: 26576\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.187240 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 199183\n",
      "[LightGBM] [Info] Number of data points in the train set: 58966, number of used features: 1020\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.549300 -> initscore=0.197841\n",
      "[LightGBM] [Info] Start training from score 0.197841\n",
      "[LightGBM] [Info] Number of positive: 32390, number of negative: 26576\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.163264 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 199327\n",
      "[LightGBM] [Info] Number of data points in the train set: 58966, number of used features: 1020\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.549300 -> initscore=0.197841\n",
      "[LightGBM] [Info] Start training from score 0.197841\n",
      "[LightGBM] [Info] Number of positive: 32390, number of negative: 26576\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.153925 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 199031\n",
      "[LightGBM] [Info] Number of data points in the train set: 58966, number of used features: 1020\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.549300 -> initscore=0.197841\n",
      "[LightGBM] [Info] Start training from score 0.197841\n"
     ]
    }
   ],
   "source": [
    "estimators = [\n",
    "    ('gbm', gbm),\n",
    "    ('ada', ada),\n",
    "    ('xgb', xgb),\n",
    "    ('lgbm', lgbm),\n",
    "    ('rf', rf),\n",
    "    ('log_reg', log_reg),\n",
    "    ('knn', knn),\n",
    "    ('dt', dt)\n",
    "]\n",
    "\n",
    "stacking = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\n",
    "stacking.fit(X_train.astype(np.float32), y_train.astype(np.float32))\n",
    "y_pred_stacking = stacking.predict(X_test.astype(np.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphs print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stampa delle confusion matrix\n",
    "plot_confusion_matrix(y_test, y_pred_gbm, \"Gradient Boosting Machine Confusion Matrix\", \"./images_Bow/confusion_matrix/gbm_confusion_matrix.png\")\n",
    "plot_confusion_matrix(y_test, y_pred_ada, \"AdaBoost Confusion Matrix\", \"./images_Bow/confusion_matrix/ada_confusion_matrix.png\")\n",
    "plot_confusion_matrix(y_test, y_pred_xgb, \"XGBoost Confusion Matrix\", \"./images_Bow/confusion_matrix/xgb_confusion_matrix.png\")\n",
    "plot_confusion_matrix(y_test, y_pred_lgbm, \"LightGBM Confusion Matrix\", \"./images_Bow/confusion_matrix/lgbm_confusion_matrix.png\")\n",
    "plot_confusion_matrix(y_test, y_pred_log_reg, \"Logistic Regression Confusion Matrix\", \"./images_Bow/confusion_matrix/logreg_confusion_matrix.png\")\n",
    "plot_confusion_matrix(y_test, y_pred_rf, \"Random Forest Confusion Matrix\", \"./images_Bow/confusion_matrix/rf_confusion_matrix.png\")\n",
    "plot_confusion_matrix(y_test, y_pred_knn, \"K-Nearest Neighbors Confusion Matrix\", \"./images_Bow/confusion_matrix/knn_confusion_matrix.png\")\n",
    "plot_confusion_matrix(y_test, y_pred_dt, \"Decision Tree Confusion Matrix\", \"./images_Bow/confusion_matrix/dt_confusion_matrix.png\")\n",
    "plot_confusion_matrix(y_test, y_pred_stacking, \"Stacking Classifier Confusion Matrix\", \"./images_Bow/confusion_matrix/stacking_confusion_matrix.png\")\n",
    "\n",
    "# Stampa delle ROC-AUC Curve\n",
    "plot_roc_auc_curve(y_test, y_pred_gbm, \"Gradient Boosting Machine\", \"./images_Bow/roc_auc/gbm_roc_auc.png\")\n",
    "plot_roc_auc_curve(y_test, y_pred_ada, \"AdaBoost\", \"./images_Bow/roc_auc/ada_roc_auc.png\")\n",
    "plot_roc_auc_curve(y_test, y_pred_xgb, \"XGBoost\", \"./images_Bow/roc_auc/xgb_roc_auc.png\")\n",
    "plot_roc_auc_curve(y_test, y_pred_lgbm, \"LightGBM\", \"./images_Bow/roc_auc/lgbm_roc_auc.png\")\n",
    "plot_roc_auc_curve(y_test, y_pred_log_reg, \"Logistic Regression\", \"./images_Bow/roc_auc/logreg_roc_auc.png\")\n",
    "plot_roc_auc_curve(y_test, y_pred_rf, \"Random Forest\", \"./images_Bow/roc_auc/rf_roc_auc.png\")\n",
    "plot_roc_auc_curve(y_test, y_pred_knn, \"K-Nearest Neighbors\", \"./images_Bow/roc_auc/knn_roc_auc.png\")\n",
    "plot_roc_auc_curve(y_test, y_pred_dt, \"Decision Tree\", \"./images_Bow/roc_auc/dt_roc_auc.png\")\n",
    "plot_roc_auc_curve(y_test, y_pred_stacking, \"Stacking Classifier\", \"./images_Bow/roc_auc/stacking_roc_auc.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raccoglie le statistiche per ogni modello, accuratazza, precisione, richiamo, f1, matrice di confusione e ROC-AUC\n",
    "models_stats = {\n",
    "    \"Gradient Boosting Machine\": evaluate_model(y_test, y_pred_gbm),\n",
    "    \"AdaBoost\": evaluate_model(y_test, y_pred_ada),\n",
    "    \"XGBoost\": evaluate_model(y_test, y_pred_xgb),\n",
    "    \"LightGBM\": evaluate_model(y_test, y_pred_lgbm),\n",
    "    \"Logistic Regression\": evaluate_model(y_test, y_pred_log_reg),\n",
    "    \"Random Forest\": evaluate_model(y_test, y_pred_rf),\n",
    "    \"K-Nearest Neighbors\": evaluate_model(y_test, y_pred_knn),\n",
    "    \"Decision Tree\": evaluate_model(y_test, y_pred_dt),\n",
    "    \"Stacking Classifier\": evaluate_model(y_test, y_pred_stacking)\n",
    "}\n",
    "\n",
    "# Creazione di un report di classificazione\n",
    "plot_classification_report(models_stats, \"./images_Bow/classification/classification_report\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+----------+-----------+--------+----------+---------+\n",
      "|          Model          | Accuracy | Precision | Recall | F1 Score | ROC AUC |\n",
      "+=========================+==========+===========+========+==========+=========+\n",
      "| Gradient Boosting       | 0.991    | 0.993     | 0.991  | 0.992    | 0.991   |\n",
      "| Machine                 |          |           |        |          |         |\n",
      "+-------------------------+----------+-----------+--------+----------+---------+\n",
      "| AdaBoost                | 0.989    | 0.990     | 0.989  | 0.990    | 0.989   |\n",
      "+-------------------------+----------+-----------+--------+----------+---------+\n",
      "| XGBoost                 | 0.996    | 0.998     | 0.996  | 0.997    | 0.996   |\n",
      "+-------------------------+----------+-----------+--------+----------+---------+\n",
      "| LightGBM                | 0.997    | 0.998     | 0.996  | 0.997    | 0.997   |\n",
      "+-------------------------+----------+-----------+--------+----------+---------+\n",
      "| Logistic Regression     | 0.986    | 0.993     | 0.981  | 0.987    | 0.987   |\n",
      "+-------------------------+----------+-----------+--------+----------+---------+\n",
      "| Random Forest           | 0.995    | 0.998     | 0.993  | 0.995    | 0.995   |\n",
      "+-------------------------+----------+-----------+--------+----------+---------+\n",
      "| K-Nearest Neighbors     | 0.969    | 0.983     | 0.959  | 0.971    | 0.970   |\n",
      "+-------------------------+----------+-----------+--------+----------+---------+\n",
      "| Decision Tree           | 0.989    | 0.989     | 0.991  | 0.990    | 0.989   |\n",
      "+-------------------------+----------+-----------+--------+----------+---------+\n",
      "| Stacking Classifier     | 0.997    | 0.998     | 0.996  | 0.997    | 0.997   |\n",
      "+-------------------------+----------+-----------+--------+----------+---------+\n"
     ]
    }
   ],
   "source": [
    "# Create a Texttable object\n",
    "table = Texttable()\n",
    "\n",
    "# Add headers\n",
    "table.header([\"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"ROC AUC\"])\n",
    "\n",
    "# Add rows for each model's statistics\n",
    "for model, stats in models_stats.items():\n",
    "    table.add_row([model, stats['accuracy'], stats['precision'], stats['recall'], stats['f1'], stats['roc_auc']])\n",
    "\n",
    "# Draw the table\n",
    "print(table.draw())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models have been saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Save each model to a file\n",
    "joblib.dump(vectorizer, './models/BoW/vectorizer.pkl')\n",
    "joblib.dump(gbm, './models/BoW/gbm_model.pkl')\n",
    "joblib.dump(ada, './models/BoW/ada_model.pkl')\n",
    "joblib.dump(xgb, './models/BoW/xgb_model.pkl')\n",
    "joblib.dump(lgbm, './models/BoW/lgbm_model.pkl')\n",
    "joblib.dump(log_reg, './models/BoW/log_reg_model.pkl')\n",
    "joblib.dump(rf, './models/BoW/rf_model.pkl')\n",
    "joblib.dump(knn, './models/BoW/knn_model.pkl')\n",
    "joblib.dump(dt, './models/BoW/dt_model.pkl')\n",
    "joblib.dump(stacking, './models/BoW/stacking_model.pkl')\n",
    "\n",
    "print(\"Models have been saved successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
